"use strict";(self.webpackChunkai_native_book=self.webpackChunkai_native_book||[]).push([[551],{4554:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vla/chapter-4-3-llm-planning","title":"Chapter 4.3: Language Models for Robot Planning","description":"Overview","source":"@site/docs/module-4-vla/chapter-4-3-llm-planning.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-4-3-llm-planning","permalink":"/ai_spec_driven_book/docs/module-4-vla/chapter-4-3-llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/talhabinhussain/ai_spec_driven_book/tree/main/docs/module-4-vla/chapter-4-3-llm-planning.mdx","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4.2: Speech Recognition with Whisper","permalink":"/ai_spec_driven_book/docs/module-4-vla/chapter-4-2-whisper-speech"},"next":{"title":"Chapter 4.4: Vision-Language Integration","permalink":"/ai_spec_driven_book/docs/module-4-vla/chapter-4-4-vision-language"}}');var a=t(4848),s=t(8453);const i={sidebar_position:4},r="Chapter 4.3: Language Models for Robot Planning",l={},c=[{value:"Overview",id:"overview",level:2},{value:"LLMs in Robotic Planning",id:"llms-in-robotic-planning",level:2},{value:"Planning Capabilities",id:"planning-capabilities",level:3},{value:"Integration with LangChain for Robotics",id:"integration-with-langchain-for-robotics",level:2},{value:"Advanced Planning with Function Calling",id:"advanced-planning-with-function-calling",level:2},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"Memory and Context Management",id:"memory-and-context-management",level:2},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Practical Example: Kitchen Assistant Robot",id:"practical-example-kitchen-assistant-robot",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Prompt Engineering",id:"1-prompt-engineering",level:3},{value:"2. Context Management",id:"2-context-management",level:3},{value:"3. Safety and Validation",id:"3-safety-and-validation",level:3},{value:"4. Performance Optimization",id:"4-performance-optimization",level:3},{value:"Common Pitfalls",id:"common-pitfalls",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function p(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-43-language-models-for-robot-planning",children:"Chapter 4.3: Language Models for Robot Planning"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Large Language Models (LLMs) have emerged as powerful tools for cognitive planning in robotics, bridging the gap between natural language commands and executable robot actions. This chapter explores how to leverage LLMs like GPT-4, Claude, and open-source alternatives to translate high-level goals into detailed action sequences. You'll learn to build intelligent planning systems that can handle complex, multi-step tasks while adapting to environmental changes."}),"\n",(0,a.jsx)(e.h2,{id:"llms-in-robotic-planning",children:"LLMs in Robotic Planning"}),"\n",(0,a.jsx)(e.p,{children:"Large Language Models excel at understanding natural language and can be prompted to generate structured plans for robotic tasks. The key insight is that LLMs can serve as a cognitive layer that decomposes high-level goals into executable steps."}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-mermaid",children:"graph LR\n    A[Natural Language Goal] --\x3e B[LLM Planner]\n    B --\x3e C[Structured Action Plan]\n    C --\x3e D[ROS 2 Action Server]\n    D --\x3e E[Robot Execution]\n    E --\x3e F[Environment Feedback]\n    F --\x3e B\n"})}),"\n",(0,a.jsx)(e.h3,{id:"planning-capabilities",children:"Planning Capabilities"}),"\n",(0,a.jsx)(e.p,{children:"LLMs can provide several key planning capabilities:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking complex goals into subtasks"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Spatial Reasoning"}),": Understanding spatial relationships and navigation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Temporal Sequencing"}),": Ordering actions in time"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Context Awareness"}),": Adapting plans based on environment state"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Error Recovery"}),": Suggesting alternatives when plans fail"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-langchain-for-robotics",children:"Integration with LangChain for Robotics"}),"\n",(0,a.jsx)(e.p,{children:"LangChain provides an excellent framework for orchestrating LLM interactions in robotic systems:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.tools import BaseTool\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass LLMPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planning_node\')\n\n        # Initialize LLM\n        self.llm = ChatOpenAI(\n            model_name="gpt-4",\n            temperature=0.1,\n            max_tokens=500\n        )\n\n        # Setup ROS 2 interfaces\n        self.command_sub = self.create_subscription(\n            String, \'high_level_command\', self.command_callback, 10\n        )\n        self.plan_pub = self.create_publisher(String, \'action_plan\', 10)\n\n        # Planning prompt template\n        self.planning_prompt = ChatPromptTemplate.from_messages([\n            ("system", self.get_system_prompt()),\n            ("user", "{user_command}"),\n            ("user", "Current robot state: {robot_state}"),\n            ("user", "Environment context: {environment_context}")\n        ])\n\n    def get_system_prompt(self):\n        """\n        System prompt defining the robot planning role\n        """\n        return """\n        You are an intelligent robot planner. Your role is to convert natural language commands into structured action plans for a robot.\n\n        Guidelines:\n        1. Break down complex commands into simple, executable actions\n        2. Consider the robot\'s current state and environment\n        3. Use these action primitives: navigate_to, pick_up, place_down, open, close, turn_on, turn_off\n        4. Include spatial reasoning and obstacle awareness\n        5. Provide error handling suggestions\n        6. Output in JSON format with action sequence\n        """\n\n    def command_callback(self, msg):\n        """\n        Process high-level commands through LLM planning\n        """\n        user_command = msg.data\n\n        # Get current robot state (simplified)\n        robot_state = self.get_robot_state()\n        environment_context = self.get_environment_context()\n\n        # Create prompt\n        prompt = self.planning_prompt.format_messages(\n            user_command=user_command,\n            robot_state=robot_state,\n            environment_context=environment_context\n        )\n\n        # Generate plan\n        response = self.llm(prompt)\n        plan = self.parse_plan_response(response.content)\n\n        # Publish structured plan\n        plan_msg = String()\n        plan_msg.data = plan\n        self.plan_pub.publish(plan_msg)\n\n        self.get_logger().info(f"Generated plan for: {user_command}")\n\n    def parse_plan_response(self, response_text):\n        """\n        Parse LLM response into structured plan\n        """\n        import json\n        try:\n            # Extract JSON from response if present\n            import re\n            json_match = re.search(r\'\\{.*\\}\', response_text, re.DOTALL)\n            if json_match:\n                plan_json = json_match.group(0)\n                plan = json.loads(plan_json)\n                return json.dumps(plan)\n        except json.JSONDecodeError:\n            pass\n\n        # If no JSON, return as plain text\n        return response_text\n\n    def get_robot_state(self):\n        """\n        Get current robot state (position, battery, etc.)\n        """\n        return {\n            "position": "kitchen",\n            "battery_level": "85%",\n            "gripper_status": "open",\n            "current_task": "idle"\n        }\n\n    def get_environment_context(self):\n        """\n        Get environment context (objects, obstacles, etc.)\n        """\n        return {\n            "objects_in_view": ["red cup", "blue bottle", "wooden table"],\n            "navigable_areas": ["kitchen", "living room", "bedroom"],\n            "obstacles": ["closed door to bedroom"]\n        }\n'})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-planning-with-function-calling",children:"Advanced Planning with Function Calling"}),"\n",(0,a.jsx)(e.p,{children:"Modern LLMs support function calling, which allows for more structured interactions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import json\nfrom typing import Dict, List\n\nclass FunctionCallingPlanner:\n    def __init__(self):\n        self.functions = [\n            {\n                "name": "navigate_to",\n                "description": "Navigate robot to a specific location",\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "location": {"type": "string", "description": "Target location"},\n                        "avoid_obstacles": {"type": "boolean", "default": True}\n                    },\n                    "required": ["location"]\n                }\n            },\n            {\n                "name": "pick_up_object",\n                "description": "Pick up an object with robot gripper",\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "object_name": {"type": "string", "description": "Name of object to pick up"},\n                        "location": {"type": "string", "description": "Location where object is located"}\n                    },\n                    "required": ["object_name"]\n                }\n            },\n            {\n                "name": "place_object",\n                "description": "Place object at specified location",\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "location": {"type": "string", "description": "Location to place object"},\n                        "object_name": {"type": "string", "description": "Name of object to place"}\n                    },\n                    "required": ["location"]\n                }\n            }\n        ]\n\n    def create_planning_request(self, user_command, robot_state, environment):\n        """\n        Create structured request for function-calling LLM\n        """\n        return {\n            "model": "gpt-4-turbo",\n            "messages": [\n                {\n                    "role": "system",\n                    "content": "You are a robot planning assistant. Use available functions to create action plans. Always verify preconditions before executing actions."\n                },\n                {\n                    "role": "user",\n                    "content": f"Command: {user_command}\\nCurrent state: {robot_state}\\nEnvironment: {environment}"\n                }\n            ],\n            "functions": self.functions,\n            "function_call": "auto"\n        }\n\n    def execute_plan_with_llm(self, user_command, robot_state, environment):\n        """\n        Execute planning with LLM function calling\n        """\n        import openai\n\n        request = self.create_planning_request(user_command, robot_state, environment)\n\n        try:\n            response = openai.ChatCompletion.create(**request)\n\n            # Process function calls\n            plan = []\n            for choice in response.choices:\n                if hasattr(choice.message, \'function_call\'):\n                    function_call = choice.message.function_call\n                    plan.append({\n                        "function": function_call.name,\n                        "arguments": json.loads(function_call.arguments)\n                    })\n\n            return plan\n        except Exception as e:\n            print(f"Error in LLM planning: {e}")\n            return []\n'})}),"\n",(0,a.jsx)(e.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,a.jsx)(e.p,{children:"Effective prompt engineering is crucial for reliable robot planning:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class RobotPromptEngineer:\n    def __init__(self):\n        self.prompt_templates = {\n            \'navigation\': self.create_navigation_prompt(),\n            \'manipulation\': self.create_manipulation_prompt(),\n            \'complex_task\': self.create_complex_task_prompt()\n        }\n\n    def create_navigation_prompt(self):\n        """\n        Create prompt template for navigation tasks\n        """\n        return """\n        Plan navigation from {start_location} to {target_location}.\n\n        Consider:\n        - Current obstacles: {obstacles}\n        - Available paths: {available_paths}\n        - Safety requirements: {safety_constraints}\n\n        Output format:\n        {{\n            "actions": [\n                {{"action": "navigate_to", "location": "...", "reason": "..."}},\n                {{"action": "check_obstacle", "location": "...", "reason": "..."}}\n            ],\n            "estimated_time": "...",\n            "safety_notes": "..."\n        }}\n        """\n\n    def create_manipulation_prompt(self):\n        """\n        Create prompt template for manipulation tasks\n        """\n        return """\n        Plan manipulation task: {task_description}\n\n        Object details:\n        - Name: {object_name}\n        - Size: {object_size}\n        - Weight: {object_weight}\n        - Location: {object_location}\n\n        Robot capabilities:\n        - Gripper type: {gripper_type}\n        - Maximum weight: {max_weight}\n        - Reach range: {reach_range}\n\n        Output format:\n        {{\n            "preparation": "...",\n            "actions": [\n                {{"action": "navigate_to", "location": "...", "reason": "..."}},\n                {{"action": "align_gripper", "orientation": "...", "reason": "..."}},\n                {{"action": "grasp", "force": "...", "reason": "..."}},\n                {{"action": "verify_grasp", "method": "...", "reason": "..."}}\n            ],\n            "safety_checks": ["..."],\n            "success_criteria": "..."\n        }}\n        """\n\n    def create_complex_task_prompt(self):\n        """\n        Create prompt template for complex multi-step tasks\n        """\n        return """\n        Plan complex task: {task_description}\n\n        Task breakdown:\n        - Subtasks: {subtasks}\n        - Dependencies: {dependencies}\n        - Resources needed: {resources}\n\n        Environment state:\n        - Objects: {objects}\n        - Locations: {locations}\n        - Constraints: {constraints}\n\n        Plan with:\n        1. Task decomposition\n        2. Resource allocation\n        3. Failure handling\n        4. Success verification\n\n        Output format:\n        {{\n            "task_breakdown": [...],\n            "action_sequence": [\n                {{"step": 1, "action": "...", "conditions": "...", "verification": "..."}}\n            ],\n            "failure_scenarios": [...],\n            "recovery_actions": [...],\n            "success_criteria": "..."\n        }}\n        """\n\n    def generate_specific_prompt(self, task_type, **kwargs):\n        """\n        Generate specific prompt based on task type\n        """\n        template = self.prompt_templates.get(task_type)\n        if template:\n            return template.format(**kwargs)\n        else:\n            return f"Plan task: {kwargs.get(\'task_description\', \'unknown\')}"\n'})}),"\n",(0,a.jsx)(e.h2,{id:"memory-and-context-management",children:"Memory and Context Management"}),"\n",(0,a.jsx)(e.p,{children:"LLMs benefit from proper context management for multi-turn planning:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ContextualPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'contextual_planning_node\')\n\n        self.llm = ChatOpenAI(model_name="gpt-4", temperature=0.1)\n\n        # Maintain conversation history\n        self.conversation_history = []\n        self.max_history_length = 10\n\n        # ROS 2 interfaces\n        self.command_sub = self.create_subscription(String, \'command\', self.command_callback, 10)\n        self.plan_pub = self.create_publisher(String, \'plan\', 10)\n        self.memory_sub = self.create_subscription(String, \'memory_update\', self.memory_callback, 10)\n\n    def command_callback(self, msg):\n        """\n        Process command with context awareness\n        """\n        user_command = msg.data\n\n        # Add to conversation history\n        self.conversation_history.append({"role": "user", "content": user_command})\n\n        # Keep history within limits\n        if len(self.conversation_history) > self.max_history_length:\n            self.conversation_history = self.conversation_history[-self.max_history_length:]\n\n        # Get environmental context\n        context = self.get_current_context()\n\n        # Create contextual prompt\n        prompt = self.create_contextual_prompt(user_command, context)\n\n        # Generate plan\n        response = self.llm([{"role": "user", "content": prompt}])\n\n        # Add to history\n        self.conversation_history.append({"role": "assistant", "content": response.content})\n\n        # Publish plan\n        plan_msg = String()\n        plan_msg.data = response.content\n        self.plan_pub.publish(plan_msg)\n\n    def get_current_context(self):\n        """\n        Get current environmental and task context\n        """\n        return {\n            "location": "current_room",\n            "objects": ["object1", "object2"],\n            "previous_tasks": self.get_recent_tasks(),\n            "robot_capabilities": self.get_robot_capabilities(),\n            "time_of_day": self.get_current_time()\n        }\n\n    def get_recent_tasks(self):\n        """\n        Get recently completed tasks\n        """\n        # In practice, this would query a task database\n        return [\n            {"task": "navigation", "result": "success", "location": "kitchen"},\n            {"task": "object_search", "result": "partial", "object": "cup"}\n        ]\n\n    def get_robot_capabilities(self):\n        """\n        Get current robot capabilities and status\n        """\n        return {\n            "battery": "85%",\n            "gripper": "functional",\n            "navigation": "enabled",\n            "manipulation": "enabled"\n        }\n\n    def get_current_time(self):\n        """\n        Get current time for context\n        """\n        import datetime\n        return datetime.datetime.now().strftime("%H:%M")\n\n    def create_contextual_prompt(self, command, context):\n        """\n        Create prompt with environmental context\n        """\n        return f"""\n        Command: {command}\n\n        Context:\n        - Current location: {context[\'location\']}\n        - Visible objects: {context[\'objects\']}\n        - Recent tasks: {context[\'previous_tasks\']}\n        - Robot status: {context[\'robot_capabilities\']}\n        - Time: {context[\'time_of_day\']}\n\n        Conversation history:\n        {self.format_conversation_history()}\n\n        Generate a plan that considers the context and previous interactions.\n        """\n\n    def format_conversation_history(self):\n        """\n        Format conversation history for context\n        """\n        history_str = ""\n        for item in self.conversation_history[-5:]:  # Last 5 exchanges\n            role = item[\'role\']\n            content = item[\'content\']\n            history_str += f"{role}: {content}\\n"\n        return history_str\n'})}),"\n",(0,a.jsx)(e.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,a.jsx)(e.p,{children:"LLMs should be prompted to consider error scenarios and recovery strategies:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class RobustPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'robust_planning_node\')\n        self.llm = ChatOpenAI(model_name="gpt-4", temperature=0.1)\n\n    def generate_robust_plan(self, user_command, context):\n        """\n        Generate plan with built-in error handling\n        """\n        prompt = f"""\n        Generate a detailed plan for: {user_command}\n\n        Context: {context}\n\n        Requirements:\n        1. Include pre-action checks\n        2. Anticipate potential failures\n        3. Provide recovery strategies\n        4. Include success verification steps\n        5. Consider safety constraints\n\n        Output format:\n        {{\n            "primary_plan": [\n                {{"step": "...", "action": "...", "precondition": "...", "verification": "..."}}\n            ],\n            "failure_scenarios": [\n                {{"failure": "...", "detection": "...", "recovery": "..."}}\n            ],\n            "safety_checks": ["..."],\n            "success_criteria": "..."\n        }}\n        """\n\n        response = self.llm([{"role": "user", "content": prompt}])\n        return response.content\n\n    def handle_plan_failure(self, failed_action, error_description):\n        """\n        Use LLM to suggest recovery from plan failure\n        """\n        recovery_prompt = f"""\n        Plan failed at action: {failed_action}\n        Error: {error_description}\n\n        Current state:\n        - Robot position: ...\n        - Object states: ...\n        - Environment: ...\n\n        Suggest recovery strategies:\n        1. Immediate actions to address the failure\n        2. Alternative approaches to complete the task\n        3. Safety considerations for recovery\n        """\n\n        response = self.llm([{"role": "user", "content": recovery_prompt}])\n        return response.content\n'})}),"\n",(0,a.jsx)(e.h2,{id:"practical-example-kitchen-assistant-robot",children:"Practical Example: Kitchen Assistant Robot"}),"\n",(0,a.jsx)(e.p,{children:"Let's build a complete example of an LLM-powered kitchen assistant:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class KitchenAssistantNode(Node):\n    def __init__(self):\n        super().__init__(\'kitchen_assistant_node\')\n\n        # LLM for planning\n        self.llm = ChatOpenAI(model_name="gpt-4", temperature=0.2)\n\n        # ROS 2 interfaces\n        self.command_sub = self.create_subscription(String, \'user_command\', self.command_callback, 10)\n        self.action_pub = self.create_publisher(String, \'robot_action\', 10)\n        self.status_sub = self.create_subscription(String, \'robot_status\', self.status_callback, 10)\n\n        # Kitchen context\n        self.kitchen_objects = {\n            "cabinet": ["plates", "cups", "bowls"],\n            "fridge": ["milk", "eggs", "cheese"],\n            "counter": ["knife", "cutting_board"],\n            "table": ["newspaper", "keys"]\n        }\n\n        self.robot_status = {\n            "location": "home_base",\n            "gripper": "open",\n            "battery": 100\n        }\n\n    def command_callback(self, msg):\n        """\n        Handle user commands for kitchen tasks\n        """\n        user_command = msg.data\n\n        # Generate plan with LLM\n        plan = self.generate_kitchen_plan(user_command)\n\n        # Execute plan\n        self.execute_plan(plan)\n\n    def generate_kitchen_plan(self, command):\n        """\n        Generate detailed kitchen task plan using LLM\n        """\n        prompt = f"""\n        You are a kitchen assistant robot. Plan the following task: {command}\n\n        Kitchen layout:\n        - Locations: home_base, counter, cabinet, fridge, table, sink\n        - Objects: {self.kitchen_objects}\n        - Current robot status: {self.robot_status}\n\n        Generate a step-by-step plan that:\n        1. Navigates to required locations\n        2. Identifies and manipulates objects\n        3. Performs kitchen-specific actions\n        4. Returns to home base when complete\n        5. Includes safety checks\n\n        Output JSON format:\n        {{\n            "task": "{command}",\n            "steps": [\n                {{"action": "navigate_to", "location": "...", "reason": "..."}},\n                {{"action": "detect_object", "object": "...", "location": "..."}},\n                {{"action": "manipulate", "object": "...", "operation": "..."}},\n                {{"action": "return_home", "reason": "task_complete"}}\n            ],\n            "safety_checks": ["..."],\n            "success_criteria": "..."\n        }}\n        """\n\n        response = self.llm([{"role": "user", "content": prompt}])\n\n        # Parse and return plan\n        import json\n        import re\n        json_match = re.search(r\'\\{.*\\}\', response.content, re.DOTALL)\n        if json_match:\n            plan = json.loads(json_match.group(0))\n            return plan\n\n        return {"error": "Failed to parse plan", "raw_response": response.content}\n\n    def execute_plan(self, plan):\n        """\n        Execute the generated plan step by step\n        """\n        if "error" in plan:\n            self.get_logger().error(f"Plan generation failed: {plan[\'error\']}")\n            return\n\n        for step in plan.get("steps", []):\n            action = step["action"]\n\n            if action == "navigate_to":\n                self.navigate_to_location(step["location"])\n            elif action == "detect_object":\n                self.detect_object(step["object"], step["location"])\n            elif action == "manipulate":\n                self.manipulate_object(step["object"], step["operation"])\n            elif action == "return_home":\n                self.return_to_home()\n\n            # Verify step completion\n            if not self.verify_step_completion(step):\n                self.get_logger().warn(f"Step failed: {step}")\n                break\n\n    def navigate_to_location(self, location):\n        """\n        Navigate to specified location\n        """\n        action_msg = String()\n        action_msg.data = f"navigate_to:{location}"\n        self.action_pub.publish(action_msg)\n        self.get_logger().info(f"Navigating to {location}")\n\n    def detect_object(self, obj, location):\n        """\n        Detect object at specified location\n        """\n        action_msg = String()\n        action_msg.data = f"detect_object:{obj}@{location}"\n        self.action_pub.publish(action_msg)\n        self.get_logger().info(f"Detecting {obj} at {location}")\n\n    def manipulate_object(self, obj, operation):\n        """\n        Manipulate object with specified operation\n        """\n        action_msg = String()\n        action_msg.data = f"manipulate:{operation}:{obj}"\n        self.action_pub.publish(action_msg)\n        self.get_logger().info(f"{operation} {obj}")\n\n    def return_to_home(self):\n        """\n        Return robot to home base\n        """\n        action_msg = String()\n        action_msg.data = "navigate_to:home_base"\n        self.action_pub.publish(action_msg)\n        self.get_logger().info("Returning to home base")\n\n    def verify_step_completion(self, step):\n        """\n        Verify that a step was completed successfully\n        """\n        # In practice, this would check robot feedback\n        # For now, assume success\n        return True\n\n    def status_callback(self, msg):\n        """\n        Update robot status from feedback\n        """\n        import json\n        try:\n            status_update = json.loads(msg.data)\n            self.robot_status.update(status_update)\n        except json.JSONDecodeError:\n            self.get_logger().warn("Failed to parse status update")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(e.h3,{id:"1-prompt-engineering",children:"1. Prompt Engineering"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use clear, specific instructions"}),"\n",(0,a.jsx)(e.li,{children:"Provide examples of desired output format"}),"\n",(0,a.jsx)(e.li,{children:"Include constraints and requirements"}),"\n",(0,a.jsx)(e.li,{children:"Test prompts with various inputs"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-context-management",children:"2. Context Management"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Limit conversation history to prevent token overflow"}),"\n",(0,a.jsx)(e.li,{children:"Include relevant environmental context"}),"\n",(0,a.jsx)(e.li,{children:"Update context based on robot feedback"}),"\n",(0,a.jsx)(e.li,{children:"Maintain task-specific memory"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-safety-and-validation",children:"3. Safety and Validation"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Always validate LLM-generated plans before execution"}),"\n",(0,a.jsx)(e.li,{children:"Implement safety checks and constraints"}),"\n",(0,a.jsx)(e.li,{children:"Include error handling in prompts"}),"\n",(0,a.jsx)(e.li,{children:"Monitor for unsafe command interpretations"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"4-performance-optimization",children:"4. Performance Optimization"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Cache frequent query patterns"}),"\n",(0,a.jsx)(e.li,{children:"Use appropriate model sizes for your needs"}),"\n",(0,a.jsx)(e.li,{children:"Implement streaming for long responses"}),"\n",(0,a.jsx)(e.li,{children:"Consider local models for sensitive applications"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"common-pitfalls",children:"Common Pitfalls"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Over-reliance on LLMs"}),": LLMs can hallucinate; always validate outputs"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Token Limitations"}),": Long contexts may exceed model limits"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Latency Issues"}),": Complex prompts can result in slow responses"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Concerns"}),": LLMs may generate unsafe plans without proper constraints"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"In this chapter, you learned:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"\u2705 How to integrate LLMs with ROS 2 for robotic planning"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Using LangChain for advanced LLM orchestration"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Function calling for structured robot interactions"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Prompt engineering techniques for robotics"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Context management and memory systems"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Error handling and recovery strategies"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Building complete LLM-powered robotic assistants"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"Now that you understand how to use LLMs for planning, let's explore how to combine vision and language understanding for more sophisticated robot capabilities."}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Continue to:"})," ",(0,a.jsx)(e.a,{href:"./chapter-4-4-vision-language",children:"Chapter 4.4: Vision-Language Integration \u2192"})]}),"\n",(0,a.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://python.langchain.com/",children:"LangChain Documentation"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/function-calling",children:"OpenAI Function Calling Guide"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://arxiv.org/abs/2305.15021",children:"Prompt Engineering for Robotics"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://arxiv.org/abs/2308.16895",children:"Large Language Models for Robotics"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://ieeexplore.ieee.org/document/9912345",children:"Robot Planning with LLMs"})}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>r});var o=t(6540);const a={},s=o.createContext(a);function i(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);