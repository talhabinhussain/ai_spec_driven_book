"use strict";(self.webpackChunkai_native_book=self.webpackChunkai_native_book||[]).push([[462],{1034:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-4-vla/chapter-4-7-capstone-project","title":"Chapter 4.7: The Capstone Project - Autonomous Humanoid","description":"Overview","source":"@site/docs/module-4-vla/chapter-4-7-capstone-project.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-4-7-capstone-project","permalink":"/ai_spec_driven_book/docs/module-4-vla/chapter-4-7-capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/talhabinhussain/ai_spec_driven_book/tree/main/docs/module-4-vla/chapter-4-7-capstone-project.mdx","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4.6: Building Conversational Robots","permalink":"/ai_spec_driven_book/docs/module-4-vla/chapter-4-6-conversational-robots"}}');var o=t(4848),i=t(8453);const a={sidebar_position:8},r="Chapter 4.7: The Capstone Project - Autonomous Humanoid",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Capstone Project Architecture",id:"capstone-project-architecture",level:2},{value:"System Components",id:"system-components",level:3},{value:"Complete System Integration",id:"complete-system-integration",level:2},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:2},{value:"LLM Planning Integration",id:"llm-planning-integration",level:2},{value:"Vision-Language Integration",id:"vision-language-integration",level:2},{value:"Action Execution and Control",id:"action-execution-and-control",level:2},{value:"Isaac Sim Integration",id:"isaac-sim-integration",level:2},{value:"System Testing and Validation",id:"system-testing-and-validation",level:2},{value:"Practical Example: Complete Autonomous Task",id:"practical-example-complete-autonomous-task",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Best Practices for VLA Integration",id:"best-practices-for-vla-integration",level:2},{value:"1. System Architecture",id:"1-system-architecture",level:3},{value:"2. Real-Time Performance",id:"2-real-time-performance",level:3},{value:"3. Safety and Reliability",id:"3-safety-and-reliability",level:3},{value:"4. User Experience",id:"4-user-experience",level:3},{value:"Common Pitfalls and Solutions",id:"common-pitfalls-and-solutions",level:2},{value:"1. Integration Challenges",id:"1-integration-challenges",level:3},{value:"2. Performance Issues",id:"2-performance-issues",level:3},{value:"3. Error Propagation",id:"3-error-propagation",level:3},{value:"4. Resource Management",id:"4-resource-management",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Project Ideas for Further Development",id:"project-ideas-for-further-development",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-47-the-capstone-project---autonomous-humanoid",children:"Chapter 4.7: The Capstone Project - Autonomous Humanoid"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project brings together all the components of Vision-Language-Action robotics into a complete autonomous humanoid system. This chapter guides you through building an integrated system where a simulated humanoid robot receives voice commands, processes them through LLM-based planning, navigates through environments, identifies objects using computer vision, and performs complex manipulation tasks. You'll create a complete pipeline that demonstrates the full potential of VLA systems."}),"\n",(0,o.jsx)(n.h2,{id:"capstone-project-architecture",children:"Capstone Project Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The complete autonomous humanoid system integrates all previous modules:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Voice Command] --\x3e B[Whisper ASR]\n    B --\x3e C[LLM Planner]\n    C --\x3e D[Vision-Language Understanding]\n    D --\x3e E[Action Primitives]\n    E --\x3e F[ROS 2 Control]\n    F --\x3e G[Isaac Sim Execution]\n    G --\x3e H[Real-world Deployment]\n    H --\x3e I[Feedback Loop]\n    I --\x3e A\n\n    J[Isaac Perception] --\x3e D\n    K[Navigation Stack] --\x3e E\n    L[Manipulation Stack] --\x3e E\n"})}),"\n",(0,o.jsx)(n.h3,{id:"system-components",children:"System Components"}),"\n",(0,o.jsx)(n.p,{children:"The capstone system consists of:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Interface"}),": Whisper-based speech recognition"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Layer"}),": LLM-based task planning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception System"}),": Vision-language object understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"}),": Robot control and manipulation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation System"}),": Autonomous mobility"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human Interface"}),": Conversational interaction"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"complete-system-integration",children:"Complete System Integration"}),"\n",(0,o.jsx)(n.p,{children:"Let's build the complete integrated system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Pose\nfrom your_interfaces.msg import VLACommand, VLAStatus  # Custom messages\n\nclass AutonomousHumanoidNode(Node):\n    def __init__(self):\n        super().__init__('autonomous_humanoid')\n\n        # Initialize all system components\n        self.voice_processor = VoiceCommandProcessor(self)\n        self.llm_planner = LLMPlanningNode(self)\n        self.vision_system = VisionLanguageNode(self)\n        self.action_manager = ActionPrimitiveManager(self)\n        self.dialogue_manager = DialogueManagerNode(self)\n        self.navigation_system = NavigationSystemNode(self)\n        self.manipulation_system = ManipulationSystemNode(self)\n\n        # ROS 2 interfaces\n        self.voice_command_sub = self.create_subscription(\n            String, 'voice_command', self.voice_command_callback, 10\n        )\n        self.vla_command_sub = self.create_subscription(\n            VLACommand, 'vla_command', self.vla_command_callback, 10\n        )\n        self.system_status_pub = self.create_publisher(VLAStatus, 'system_status', 10)\n        self.robot_response_pub = self.create_publisher(String, 'robot_response', 10)\n\n        # System state\n        self.system_state = {\n            'current_task': None,\n            'task_status': 'idle',\n            'battery_level': 100,\n            'location': 'home_base',\n            'held_object': None,\n            'conversation_active': False\n        }\n\n        # Task queue for handling multiple requests\n        self.task_queue = []\n        self.current_task = None\n\n        self.get_logger().info(\"Autonomous humanoid system initialized\")\n\n    def voice_command_callback(self, msg):\n        \"\"\"\n        Main entry point for voice commands\n        \"\"\"\n        command = msg.data\n        self.get_logger().info(f\"Received voice command: {command}\")\n\n        # Update system status\n        status_msg = VLAStatus()\n        status_msg.command_received = command\n        status_msg.system_state = str(self.system_state)\n        self.system_status_pub.publish(status_msg)\n\n        # Process through VLA pipeline\n        asyncio.create_task(self.process_voice_command_async(command))\n\n    async def process_voice_command_async(self, command):\n        \"\"\"\n        Process voice command through complete VLA pipeline\n        \"\"\"\n        try:\n            # Step 1: Natural language understanding\n            intent = self.extract_intent(command)\n            entities = self.extract_entities(command)\n\n            # Step 2: Task planning with LLM\n            plan = await self.generate_plan(command, intent, entities)\n\n            if plan:\n                # Step 3: Execute plan\n                success = await self.execute_plan(plan)\n\n                # Step 4: Generate response\n                response = self.generate_response(success, command, plan)\n                self.publish_response(response)\n\n                # Update system state\n                self.update_system_state_after_execution(success, plan)\n            else:\n                response = \"I couldn't understand how to perform that task.\"\n                self.publish_response(response)\n\n        except Exception as e:\n            self.get_logger().error(f\"Error processing voice command: {e}\")\n            response = \"I encountered an error processing your request. Could you try again?\"\n            self.publish_response(response)\n\n    def extract_intent(self, command):\n        \"\"\"\n        Extract intent from natural language command\n        \"\"\"\n        command_lower = command.lower()\n\n        if any(word in command_lower for word in ['go to', 'navigate to', 'move to', 'walk to']):\n            return 'navigation'\n        elif any(word in command_lower for word in ['pick up', 'get', 'take', 'grasp']):\n            return 'manipulation'\n        elif any(word in command_lower for word in ['find', 'look for', 'search for']):\n            return 'search'\n        elif any(word in command_lower for word in ['bring', 'deliver', 'carry']):\n            return 'delivery'\n        elif any(word in command_lower for word in ['hello', 'hi', 'hey']):\n            return 'greeting'\n        else:\n            return 'unknown'\n\n    def extract_entities(self, command):\n        \"\"\"\n        Extract entities from command\n        \"\"\"\n        entities = {}\n\n        # Location entities\n        locations = ['kitchen', 'living room', 'bedroom', 'office', 'bathroom', 'home base', 'dining room']\n        for loc in locations:\n            if loc in command.lower():\n                entities['location'] = loc\n\n        # Object entities\n        objects = ['cup', 'bottle', 'book', 'phone', 'keys', 'ball', 'toy', 'plate', 'fork', 'spoon']\n        for obj in objects:\n            if obj in command.lower():\n                entities['object'] = obj\n\n        return entities\n\n    async def generate_plan(self, command, intent, entities):\n        \"\"\"\n        Generate execution plan using LLM\n        \"\"\"\n        # Create context for planning\n        context = {\n            'current_location': self.system_state['location'],\n            'battery_level': self.system_state['battery_level'],\n            'held_object': self.system_state['held_object'],\n            'environment_objects': await self.get_visible_objects(),\n            'robot_capabilities': self.get_robot_capabilities()\n        }\n\n        # Generate plan through LLM\n        plan = await self.llm_planner.generate_plan(command, context)\n        return plan\n\n    async def get_visible_objects(self):\n        \"\"\"\n        Get objects visible to robot's cameras\n        \"\"\"\n        # In practice, this would query the vision system\n        # For simulation, return some objects\n        return ['cup', 'bottle', 'book']\n\n    def get_robot_capabilities(self):\n        \"\"\"\n        Get robot's current capabilities\n        \"\"\"\n        return {\n            'navigation': True,\n            'manipulation': True,\n            'grasping': True,\n            'speech': True,\n            'vision': True\n        }\n\n    async def execute_plan(self, plan):\n        \"\"\"\n        Execute the generated plan step by step\n        \"\"\"\n        self.system_state['task_status'] = 'executing'\n\n        for step in plan.get('steps', []):\n            action = step['action']\n            parameters = step.get('parameters', {})\n\n            self.get_logger().info(f\"Executing step: {action} with {parameters}\")\n\n            # Execute action\n            success = await self.action_manager.execute_action(action, parameters)\n\n            if not success:\n                self.get_logger().warn(f\"Action failed: {action}\")\n\n                # Try recovery\n                recovery_success = await self.attempt_recovery(action, parameters)\n                if not recovery_success:\n                    self.system_state['task_status'] = 'failed'\n                    return False\n\n            # Update status\n            status_msg = VLAStatus()\n            status_msg.current_action = action\n            status_msg.action_status = 'completed' if success else 'failed'\n            self.system_status_pub.publish(status_msg)\n\n        self.system_state['task_status'] = 'completed'\n        return True\n\n    async def attempt_recovery(self, failed_action, parameters):\n        \"\"\"\n        Attempt to recover from failed action\n        \"\"\"\n        self.get_logger().info(f\"Attempting recovery for failed action: {failed_action}\")\n\n        # Simple recovery strategies\n        if failed_action == 'navigate_to':\n            # Try alternative navigation\n            return await self.navigation_system.try_alternative_navigation(parameters)\n        elif failed_action == 'grasp_object':\n            # Try different grasp approach\n            return await self.manipulation_system.try_different_grasp(parameters)\n        else:\n            # Retry with different parameters\n            return await self.action_manager.execute_action(failed_action, parameters)\n\n    def generate_response(self, success, original_command, plan):\n        \"\"\"\n        Generate natural language response\n        \"\"\"\n        if success:\n            if plan.get('intent') == 'navigation':\n                location = plan.get('parameters', {}).get('location', 'destination')\n                return f\"I've successfully navigated to the {location}.\"\n            elif plan.get('intent') == 'manipulation':\n                obj = plan.get('parameters', {}).get('object', 'object')\n                return f\"I've successfully picked up the {obj}.\"\n            else:\n                return \"I've completed the task successfully.\"\n        else:\n            return f\"I had trouble completing the task: {original_command}\"\n\n    def publish_response(self, response):\n        \"\"\"\n        Publish response through all channels\n        \"\"\"\n        response_msg = String()\n        response_msg.data = response\n        self.robot_response_pub.publish(response_msg)\n\n        self.get_logger().info(f\"Response: {response}\")\n\n    def update_system_state_after_execution(self, success, plan):\n        \"\"\"\n        Update system state after task execution\n        \"\"\"\n        if success:\n            if plan.get('intent') == 'navigation' and 'location' in plan.get('parameters', {}):\n                self.system_state['location'] = plan['parameters']['location']\n            elif plan.get('intent') == 'manipulation':\n                obj = plan.get('parameters', {}).get('object')\n                if obj:\n                    self.system_state['held_object'] = obj\n\n    def vla_command_callback(self, msg):\n        \"\"\"\n        Handle direct VLA commands (bypassing voice processing)\n        \"\"\"\n        command_type = msg.command_type\n        parameters = msg.parameters\n\n        self.get_logger().info(f\"Received VLA command: {command_type}\")\n\n        # Execute based on command type\n        if command_type == 'direct_navigation':\n            asyncio.create_task(self.execute_direct_navigation(parameters))\n        elif command_type == 'object_interaction':\n            asyncio.create_task(self.execute_object_interaction(parameters))\n        elif command_type == 'status_request':\n            self.publish_system_status()\n\n    async def execute_direct_navigation(self, parameters):\n        \"\"\"\n        Execute direct navigation command\n        \"\"\"\n        location = parameters.get('location', 'home_base')\n        success = await self.navigation_system.navigate_to(location)\n\n        response = f\"Navigation to {location}: {'successful' if success else 'failed'}\"\n        self.publish_response(response)\n\n    async def execute_object_interaction(self, parameters):\n        \"\"\"\n        Execute object interaction command\n        \"\"\"\n        action = parameters.get('action', 'grasp')\n        obj = parameters.get('object', 'unknown')\n\n        if action == 'grasp':\n            success = await self.manipulation_system.grasp_object(obj)\n        elif action == 'place':\n            success = await self.manipulation_system.place_object(obj)\n        else:\n            success = False\n\n        response = f\"{action.capitalize()} {obj}: {'successful' if success else 'failed'}\"\n        self.publish_response(response)\n\n    def publish_system_status(self):\n        \"\"\"\n        Publish current system status\n        \"\"\"\n        status_msg = VLAStatus()\n        status_msg.current_task = str(self.system_state['current_task'])\n        status_msg.task_status = self.system_state['task_status']\n        status_msg.battery_level = self.system_state['battery_level']\n        status_msg.location = self.system_state['location']\n        status_msg.held_object = str(self.system_state['held_object'])\n\n        self.system_status_pub.publish(status_msg)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The complete voice processing pipeline:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class VoiceCommandProcessor:\n    def __init__(self, node):\n        self.node = node\n\n        # Initialize Whisper for ASR\n        import whisper\n        self.whisper_model = whisper.load_model("base.en")\n\n        # Initialize audio processing\n        import pyaudio\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=16000,\n            input=True,\n            frames_per_buffer=8192\n        )\n\n    def transcribe_audio(self, audio_data):\n        """\n        Transcribe audio data using Whisper\n        """\n        import numpy as np\n        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n        result = self.whisper_model.transcribe(audio_array)\n        return result["text"].strip()\n\n    def process_voice_input(self, audio_msg):\n        """\n        Process incoming audio message\n        """\n        try:\n            text = self.transcribe_audio(audio_msg.data)\n            if text:\n                # Publish as voice command\n                command_msg = String()\n                command_msg.data = text\n                self.node.voice_command_sub.publish(command_msg)\n                self.node.get_logger().info(f"Transcribed: {text}")\n        except Exception as e:\n            self.node.get_logger().error(f"Error in voice processing: {e}")\n\n    def start_continuous_listening(self):\n        """\n        Start continuous voice command listening\n        """\n        def listen_thread():\n            while rclpy.ok():\n                try:\n                    # Read audio chunk\n                    data = self.stream.read(8192)\n\n                    # Simple voice activity detection\n                    audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n                    if np.max(np.abs(audio_array)) > 0.01:  # Threshold for voice detection\n                        # Accumulate more audio for better transcription\n                        full_buffer = data\n                        for _ in range(3):  # Collect more samples\n                            more_data = self.stream.read(8192)\n                            full_buffer += more_data\n\n                        text = self.transcribe_audio(full_buffer)\n                        if text and len(text) > 3:  # Filter out short noise\n                            command_msg = String()\n                            command_msg.data = text\n                            self.node.voice_command_sub.publish(command_msg)\n                            self.node.get_logger().info(f"Voice detected: {text}")\n\n                except Exception as e:\n                    self.node.get_logger().error(f"Error in continuous listening: {e}")\n\n        import threading\n        threading.Thread(target=listen_thread, daemon=True).start()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"llm-planning-integration",children:"LLM Planning Integration"}),"\n",(0,o.jsx)(n.p,{children:"Advanced LLM-based planning for the humanoid:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nimport json\n\nclass LLMPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_planning_node\')\n\n        # Initialize LLM\n        self.llm = ChatOpenAI(\n            model_name="gpt-4",\n            temperature=0.1,\n            max_tokens=500\n        )\n\n        # Define system capabilities\n        self.robot_capabilities = {\n            \'navigation\': {\n                \'locations\': [\'kitchen\', \'living room\', \'bedroom\', \'office\', \'bathroom\', \'home base\'],\n                \'max_distance\': 10.0  # meters\n            },\n            \'manipulation\': {\n                \'max_weight\': 1.0,  # kg\n                \'reachable_height\': [0.2, 1.5]  # meters\n            },\n            \'perception\': {\n                \'detection_range\': 3.0,  # meters\n                \'recognizable_objects\': [\'cup\', \'bottle\', \'book\', \'phone\', \'keys\', \'ball\', \'toy\']\n            }\n        }\n\n    async def generate_plan(self, command, context):\n        """\n        Generate detailed execution plan using LLM\n        """\n        system_prompt = self.create_system_prompt()\n        user_prompt = self.create_user_prompt(command, context)\n\n        try:\n            messages = [\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_prompt}\n            ]\n\n            response = self.llm(messages)\n\n            # Parse the response into a structured plan\n            plan = self.parse_plan_response(response.content)\n            return plan\n\n        except Exception as e:\n            self.get_logger().error(f"LLM planning error: {e}")\n            return None\n\n    def create_system_prompt(self):\n        """\n        Create system prompt for LLM planning\n        """\n        return f"""\n        You are an intelligent robot planner for a humanoid robot. Your role is to convert natural language commands into detailed action plans.\n\n        Robot Capabilities:\n        - Navigation: Can move between locations: {self.robot_capabilities[\'navigation\'][\'locations\']}\n        - Manipulation: Can grasp objects up to {self.robot_capabilities[\'manipulation\'][\'max_weight\']}kg\n        - Perception: Can detect objects within {self.robot_capabilities[\'perception\'][\'detection_range\']}m\n\n        Guidelines:\n        1. Break down complex commands into simple, executable actions\n        2. Consider robot\'s current state and environment\n        3. Use these action primitives: navigate_to, detect_object, grasp_object, place_object, speak\n        4. Include spatial reasoning and obstacle awareness\n        5. Provide error handling suggestions\n        6. Output in JSON format with complete action sequence\n\n        Safety Requirements:\n        - Always verify object weight before grasping\n        - Check navigation path for obstacles\n        - Confirm object location before manipulation\n        """\n\n    def create_user_prompt(self, command, context):\n        """\n        Create user prompt with command and context\n        """\n        return f"""\n        Command: {command}\n\n        Current Context:\n        - Robot Location: {context[\'current_location\']}\n        - Battery Level: {context[\'battery_level\']}%\n        - Held Object: {context[\'held_object\']}\n        - Visible Objects: {context[\'environment_objects\']}\n        - Robot Capabilities: {context[\'robot_capabilities\']}\n\n        Generate a detailed plan that:\n        1. Considers the current state\n        2. Breaks task into executable steps\n        3. Includes safety checks\n        4. Specifies parameters for each action\n        5. Handles potential failures\n\n        Output JSON format:\n        {{\n            "intent": "navigation|manipulation|search|delivery",\n            "parameters": {{"location": "...", "object": "..."}},\n            "steps": [\n                {{"action": "navigate_to", "parameters": {{"location": "..."}}, "reason": "..."}},\n                {{"action": "detect_object", "parameters": {{"object": "..."}}, "reason": "..."}},\n                {{"action": "grasp_object", "parameters": {{"object": "..."}}, "reason": "..."}}\n            ],\n            "safety_checks": ["..."],\n            "success_criteria": "..."\n        }}\n        """\n\n    def parse_plan_response(self, response_text):\n        """\n        Parse LLM response into structured plan\n        """\n        import re\n        import json\n\n        # Extract JSON from response\n        json_match = re.search(r\'\\{.*\\}\', response_text, re.DOTALL)\n        if json_match:\n            try:\n                plan = json.loads(json_match.group(0))\n                return plan\n            except json.JSONDecodeError:\n                pass\n\n        # If no JSON, return as simple plan\n        return {\n            "intent": "unknown",\n            "steps": [{"action": "speak", "parameters": {"text": response_text}}],\n            "success_criteria": "response_delivered"\n        }\n'})}),"\n",(0,o.jsx)(n.h2,{id:"vision-language-integration",children:"Vision-Language Integration"}),"\n",(0,o.jsx)(n.p,{children:"Complete vision-language system for object identification and scene understanding:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import torch\nimport clip\nfrom PIL import Image\nimport numpy as np\nfrom cv_bridge import CvBridge\n\nclass VisionLanguageNode(Node):\n    def __init__(self):\n        super().__init__(\'vision_language_node\')\n\n        # Initialize CLIP model\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.clip_model, self.preprocess = clip.load("ViT-B/32", device=self.device)\n\n        # Initialize bridge for image conversion\n        self.bridge = CvBridge()\n\n        # ROS 2 interfaces\n        self.image_sub = self.create_subscription(\n            Image, \'camera/rgb/image_raw\', self.image_callback, 10\n        )\n        self.object_query_sub = self.create_subscription(\n            String, \'object_query\', self.object_query_callback, 10\n        )\n        self.scene_description_pub = self.create_publisher(String, \'scene_description\', 10)\n        self.object_detection_pub = self.create_publisher(String, \'object_detections\', 10)\n\n        # Store current image for queries\n        self.current_image = None\n        self.image_timestamp = None\n\n    def image_callback(self, msg):\n        """\n        Process incoming camera images\n        """\n        try:\n            # Convert ROS image to PIL Image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            pil_image = Image.fromarray(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))\n\n            # Preprocess and store\n            self.current_image = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n            self.image_timestamp = msg.header.stamp\n\n            self.get_logger().info("Image received and processed")\n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {e}")\n\n    def object_query_callback(self, msg):\n        """\n        Process object queries against current image\n        """\n        if self.current_image is not None:\n            query = msg.data\n            result = self.process_object_query(query)\n\n            result_msg = String()\n            result_msg.data = json.dumps(result)\n            self.object_detection_pub.publish(result_msg)\n        else:\n            self.get_logger().warn("No current image for object query")\n\n    def process_object_query(self, query):\n        """\n        Process object identification query using CLIP\n        """\n        # Common object detection\n        if query.startswith("find "):\n            obj_to_find = query[5:]  # Remove "find " prefix\n            return self.find_specific_object(obj_to_find)\n        elif query == "describe scene":\n            return self.describe_scene()\n        elif query.startswith("count "):\n            obj_to_count = query[6:]  # Remove "count " prefix\n            return self.count_objects(obj_to_count)\n        else:\n            # General object identification\n            return self.identify_objects_in_scene()\n\n    def find_specific_object(self, object_name):\n        """\n        Find a specific object in the current image\n        """\n        text_descriptions = [\n            f"a photo of {object_name}",\n            f"an image containing {object_name}",\n            f"{object_name} in the scene",\n            f"a clear view of {object_name}"\n        ]\n\n        text_tokens = clip.tokenize(text_descriptions).to(self.device)\n\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(self.current_image)\n            text_features = self.clip_model.encode_text(text_tokens)\n\n            logits_per_image, logits_per_text = self.clip_model(self.current_image, text_tokens)\n            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\n        # Calculate average probability across descriptions\n        avg_prob = np.mean(probs[0])\n\n        return {\n            "object": object_name,\n            "found": avg_prob > 0.3,\n            "confidence": float(avg_prob),\n            "action": "detect_object" if avg_prob > 0.3 else "object_not_found"\n        }\n\n    def describe_scene(self):\n        """\n        Generate scene description\n        """\n        scene_descriptions = [\n            "a kitchen with appliances and counters",\n            "a living room with furniture",\n            "a bedroom with bed and dresser",\n            "an office with desk and computer",\n            "a hallway with doors",\n            "a bathroom with fixtures"\n        ]\n\n        text_tokens = clip.tokenize(scene_descriptions).to(self.device)\n\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(self.current_image)\n            text_features = self.clip_model.encode_text(text_tokens)\n\n            logits_per_image, logits_per_text = self.clip_model(self.current_image, text_tokens)\n            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\n        # Find best matching scene description\n        best_match_idx = np.argmax(probs[0])\n        best_match = scene_descriptions[best_match_idx]\n        confidence = float(probs[0][best_match_idx])\n\n        return {\n            "scene_type": best_match,\n            "confidence": confidence,\n            "action": "scene_understood"\n        }\n\n    def count_objects(self, object_type):\n        """\n        Estimate count of specific object type\n        """\n        count_prompts = [\n            f"an image with one {object_type}",\n            f"an image with two {object_type}s",\n            f"an image with three {object_type}s",\n            f"an image with multiple {object_type}s"\n        ]\n\n        text_tokens = clip.tokenize(count_prompts).to(self.device)\n\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(self.current_image)\n            text_features = self.clip_model.encode_text(text_tokens)\n\n            logits_per_image, logits_per_text = self.clip_model(self.current_image, text_tokens)\n            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\n        best_prompt_idx = np.argmax(probs[0])\n        count_estimates = ["1", "2", "3", "multiple"]\n        estimated_count = count_estimates[best_prompt_idx]\n        confidence = float(probs[0][best_prompt_idx])\n\n        return {\n            "object_type": object_type,\n            "estimated_count": estimated_count,\n            "confidence": confidence,\n            "action": "count_completed"\n        }\n\n    def identify_objects_in_scene(self):\n        """\n        Identify common objects in the scene\n        """\n        common_objects = [\n            "chair", "table", "cup", "bottle", "book", "phone",\n            "computer", "lamp", "plant", "door", "window", "sofa"\n        ]\n\n        text_descriptions = [f"a photo of {obj}" for obj in common_objects]\n        text_tokens = clip.tokenize(text_descriptions).to(self.device)\n\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(self.current_image)\n            text_features = self.clip_model.encode_text(text_tokens)\n\n            logits_per_image, logits_per_text = self.clip_model(self.current_image, text_tokens)\n            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\n        # Get objects with high confidence\n        detected_objects = []\n        for i, obj in enumerate(common_objects):\n            if probs[0][i] > 0.2:\n                detected_objects.append({\n                    "object": obj,\n                    "confidence": float(probs[0][i])\n                })\n\n        # Sort by confidence\n        detected_objects.sort(key=lambda x: x["confidence"], reverse=True)\n\n        return {\n            "detected_objects": detected_objects[:5],  # Top 5 objects\n            "action": "objects_identified"\n        }\n'})}),"\n",(0,o.jsx)(n.h2,{id:"action-execution-and-control",children:"Action Execution and Control"}),"\n",(0,o.jsx)(n.p,{children:"Complete action execution system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n\nclass ActionExecutionNode(Node):\n    def __init__(self):\n        super().__init__(\'action_execution_node\')\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n        self.trajectory_client = ActionClient(\n            self, FollowJointTrajectory, \'joint_trajectory_controller/follow_joint_trajectory\'\n        )\n        self.gripper_client = ActionClient(\n            self, FollowJointTrajectory, \'gripper_controller/follow_joint_trajectory\'\n        )\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.joint_pub = self.create_publisher(JointState, \'/joint_commands\', 10)\n\n        # Subscribers\n        self.action_sub = self.create_subscription(\n            String, \'action_command\', self.action_callback, 10\n        )\n        self.status_pub = self.create_publisher(String, \'action_status\', 10)\n\n        # Action primitives\n        self.action_primitives = {\n            \'navigate_to\': self.execute_navigate_to,\n            \'grasp_object\': self.execute_grasp_object,\n            \'place_object\': self.execute_place_object,\n            \'speak\': self.execute_speak,\n            \'detect_object\': self.execute_detect_object\n        }\n\n    def action_callback(self, msg):\n        """\n        Execute action command\n        """\n        try:\n            # Parse action command (format: action_name:json_params)\n            parts = msg.data.split(\':\', 1)\n            if len(parts) >= 2:\n                action_name = parts[0]\n                params_str = parts[1]\n\n                import json\n                parameters = json.loads(params_str)\n\n                if action_name in self.action_primitives:\n                    # Execute asynchronously\n                    asyncio.create_task(\n                        self.action_primitives[action_name](parameters)\n                    )\n                else:\n                    self.get_logger().error(f"Unknown action: {action_name}")\n        except Exception as e:\n            self.get_logger().error(f"Error parsing action command: {e}")\n\n    async def execute_navigate_to(self, parameters):\n        """\n        Execute navigation action\n        """\n        location = parameters.get(\'location\', \'home_base\')\n\n        # Convert location to pose (in practice, this would come from a map)\n        pose = self.get_pose_for_location(location)\n        if not pose:\n            self.publish_status(f"navigation_failed:unknown_location:{location}")\n            return False\n\n        # Create and send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose = pose\n\n        # Wait for navigation server\n        self.nav_client.wait_for_server()\n\n        try:\n            goal_handle = await self.nav_client.send_goal_async(goal_msg)\n\n            if not goal_handle.accepted:\n                self.publish_status(f"navigation_failed:goal_rejected:{location}")\n                return False\n\n            # Wait for result with timeout\n            import rclpy.time\n            start_time = self.get_clock().now()\n            timeout = rclpy.time.Duration(seconds=30)  # 30 second timeout\n\n            while not goal_handle.is_done:\n                current_time = self.get_clock().now()\n                if (current_time - start_time) > timeout:\n                    goal_handle.cancel_goal()\n                    self.publish_status(f"navigation_failed:timeout:{location}")\n                    return False\n\n                await asyncio.sleep(0.1)  # Check every 100ms\n\n            result = await goal_handle.get_result_async()\n\n            if result.result.status == 3:  # SUCCEEDED\n                self.system_state[\'location\'] = location\n                self.publish_status(f"navigation_success:{location}")\n                return True\n            else:\n                self.publish_status(f"navigation_failed:execution_error:{location}")\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f"Navigation error: {e}")\n            self.publish_status(f"navigation_failed:exception:{str(e)}")\n            return False\n\n    def get_pose_for_location(self, location_name):\n        """\n        Get pose for named location\n        """\n        location_poses = {\n            \'kitchen\': Pose(\n                position=Point(x=1.0, y=0.0, z=0.0),\n                orientation=Quaternion(x=0.0, y=0.0, z=0.0, w=1.0)\n            ),\n            \'living_room\': Pose(\n                position=Point(x=0.0, y=1.0, z=0.0),\n                orientation=Quaternion(x=0.0, y=0.0, z=0.0, w=1.0)\n            ),\n            \'bedroom\': Pose(\n                position=Point(x=2.0, y=1.0, z=0.0),\n                orientation=Quaternion(x=0.0, y=0.0, z=0.0, w=1.0)\n            ),\n            \'home_base\': Pose(\n                position=Point(x=0.0, y=0.0, z=0.0),\n                orientation=Quaternion(x=0.0, y=0.0, z=0.0, w=1.0)\n            )\n        }\n\n        if location_name in location_poses:\n            return location_poses[location_name]\n        else:\n            self.get_logger().warn(f"Unknown location: {location_name}")\n            return None\n\n    async def execute_grasp_object(self, parameters):\n        """\n        Execute object grasping action\n        """\n        obj = parameters.get(\'object\', \'unknown\')\n\n        # First, ensure we\'re near the object\n        # This would involve navigation and object detection in practice\n\n        # Move arm to object position\n        success = await self.move_arm_to_object_position(obj)\n        if not success:\n            self.publish_status(f"grasp_failed:arm_positioning:{obj}")\n            return False\n\n        # Close gripper\n        gripper_success = await self.close_gripper()\n        if gripper_success:\n            self.system_state[\'held_object\'] = obj\n            self.publish_status(f"grasp_success:{obj}")\n            return True\n        else:\n            self.publish_status(f"grasp_failed:gripper_error:{obj}")\n            return False\n\n    async def move_arm_to_object_position(self, obj):\n        """\n        Move arm to position for grasping object\n        """\n        # In practice, this would use inverse kinematics and object position\n        # For simulation, use a predefined trajectory\n        trajectory = JointTrajectory()\n        trajectory.joint_names = [\'arm_joint_1\', \'arm_joint_2\', \'arm_joint_3\']\n\n        point = JointTrajectoryPoint()\n        point.positions = [0.5, 0.3, 0.2]  # Example positions\n        point.time_from_start.sec = 2\n\n        trajectory.points = [point]\n\n        goal_msg = FollowJointTrajectory.Goal()\n        goal_msg.trajectory = trajectory\n\n        self.trajectory_client.wait_for_server()\n        goal_handle = await self.trajectory_client.send_goal_async(goal_msg)\n\n        if not goal_handle.accepted:\n            return False\n\n        result = await goal_handle.get_result_async()\n        return result.result.error_code == 0\n\n    async def close_gripper(self):\n        """\n        Close robot gripper\n        """\n        trajectory = JointTrajectory()\n        trajectory.joint_names = [\'gripper_joint\']\n\n        point = JointTrajectoryPoint()\n        point.positions = [0.01]  # Closed position\n        point.effort = [50.0]  # Gripping force\n        point.time_from_start.sec = 1\n\n        trajectory.points = [point]\n\n        goal_msg = FollowJointTrajectory.Goal()\n        goal_msg.trajectory = trajectory\n\n        self.gripper_client.wait_for_server()\n        goal_handle = await self.gripper_client.send_goal_async(goal_msg)\n\n        if not goal_handle.accepted:\n            return False\n\n        result = await goal_handle.get_result_async()\n        return result.result.error_code == 0\n\n    def publish_status(self, status_message):\n        """\n        Publish action status\n        """\n        status_msg = String()\n        status_msg.data = status_message\n        self.status_pub.publish(status_msg)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"isaac-sim-integration",children:"Isaac Sim Integration"}),"\n",(0,o.jsx)(n.p,{children:"Integration with Isaac Sim for simulation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class IsaacSimIntegrationNode(Node):\n    def __init__(self):\n        super().__init__('isaac_sim_integration')\n\n        # ROS 2 interfaces for Isaac Sim\n        self.isaac_command_pub = self.create_publisher(String, '/isaac_sim/commands', 10)\n        self.isaac_state_sub = self.create_subscription(\n            String, '/isaac_sim/state', self.isaac_state_callback, 10\n        )\n\n        # Simulation state\n        self.simulation_state = {\n            'robot_pose': None,\n            'object_states': {},\n            'environment_state': 'running'\n        }\n\n    def send_to_isaac(self, command, parameters=None):\n        \"\"\"\n        Send command to Isaac Sim\n        \"\"\"\n        cmd_msg = String()\n        cmd_msg.data = json.dumps({\n            'command': command,\n            'parameters': parameters or {}\n        })\n        self.isaac_command_pub.publish(cmd_msg)\n\n    def isaac_state_callback(self, msg):\n        \"\"\"\n        Process state updates from Isaac Sim\n        \"\"\"\n        try:\n            state_data = json.loads(msg.data)\n            self.simulation_state.update(state_data)\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Error parsing Isaac Sim state\")\n\n    def start_simulation_task(self, task_description):\n        \"\"\"\n        Start a simulation task in Isaac Sim\n        \"\"\"\n        self.send_to_isaac('start_task', {\n            'task': task_description,\n            'environment': 'default',\n            'duration_limit': 300  # 5 minutes\n        })\n\n    def reset_simulation(self):\n        \"\"\"\n        Reset simulation to initial state\n        \"\"\"\n        self.send_to_isaac('reset_simulation')\n\n    def capture_simulation_data(self):\n        \"\"\"\n        Capture simulation data for training\n        \"\"\"\n        self.send_to_isaac('capture_data', {\n            'data_type': 'robot_trajectory',\n            'format': 'rosbag'\n        })\n"})}),"\n",(0,o.jsx)(n.h2,{id:"system-testing-and-validation",children:"System Testing and Validation"}),"\n",(0,o.jsx)(n.p,{children:"Comprehensive testing for the autonomous humanoid:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class SystemTester:\n    def __init__(self, node):\n        self.node = node\n        self.test_results = []\n        self.test_scenarios = self.define_test_scenarios()\n\n    def define_test_scenarios(self):\n        \"\"\"\n        Define comprehensive test scenarios\n        \"\"\"\n        return [\n            {\n                'name': 'simple_navigation',\n                'command': 'Go to the kitchen',\n                'expected_actions': ['navigate_to'],\n                'expected_location': 'kitchen'\n            },\n            {\n                'name': 'object_grasping',\n                'command': 'Pick up the red cup',\n                'expected_actions': ['detect_object', 'navigate_to', 'grasp_object'],\n                'expected_held_object': 'cup'\n            },\n            {\n                'name': 'complex_task',\n                'command': 'Go to the kitchen, find the bottle, and bring it to me',\n                'expected_actions': ['navigate_to', 'detect_object', 'grasp_object', 'navigate_to'],\n                'expected_held_object': 'bottle'\n            },\n            {\n                'name': 'conversational',\n                'command': 'Hello, can you help me?',\n                'expected_actions': ['speak'],\n                'expected_response': 'greeting'\n            }\n        ]\n\n    async def run_comprehensive_test(self):\n        \"\"\"\n        Run comprehensive system test\n        \"\"\"\n        self.get_logger().info(\"Starting comprehensive system test...\")\n\n        for scenario in self.test_scenarios:\n            self.get_logger().info(f\"Testing scenario: {scenario['name']}\")\n\n            # Reset system state\n            self.reset_system_state()\n\n            # Execute test scenario\n            result = await self.execute_test_scenario(scenario)\n            self.test_results.append(result)\n\n            # Log result\n            status = \"PASSED\" if result['success'] else \"FAILED\"\n            self.get_logger().info(f\"Scenario {scenario['name']}: {status}\")\n\n        # Generate test report\n        self.generate_test_report()\n\n    async def execute_test_scenario(self, scenario):\n        \"\"\"\n        Execute a single test scenario\n        \"\"\"\n        start_time = time.time()\n\n        try:\n            # Send command to system\n            command_msg = String()\n            command_msg.data = scenario['command']\n            self.node.voice_command_sub.publish(command_msg)\n\n            # Wait for execution to complete (with timeout)\n            await self.wait_for_task_completion(timeout=60)\n\n            # Verify expected outcomes\n            success = self.verify_test_outcomes(scenario)\n\n            return {\n                'scenario': scenario['name'],\n                'success': success,\n                'duration': time.time() - start_time,\n                'expected': scenario.get('expected_actions', []),\n                'actual': self.get_recent_actions(),\n                'error': None\n            }\n\n        except Exception as e:\n            return {\n                'scenario': scenario['name'],\n                'success': False,\n                'duration': time.time() - start_time,\n                'expected': scenario.get('expected_actions', []),\n                'actual': [],\n                'error': str(e)\n            }\n\n    def verify_test_outcomes(self, scenario):\n        \"\"\"\n        Verify that test outcomes match expectations\n        \"\"\"\n        success = True\n\n        # Check expected actions were performed\n        if 'expected_actions' in scenario:\n            actual_actions = self.get_recent_actions()\n            for expected_action in scenario['expected_actions']:\n                if expected_action not in actual_actions:\n                    self.get_logger().warn(f\"Expected action {expected_action} not found in {actual_actions}\")\n                    success = False\n\n        # Check expected location\n        if 'expected_location' in scenario:\n            if self.node.system_state['location'] != scenario['expected_location']:\n                self.get_logger().warn(f\"Expected location {scenario['expected_location']}, got {self.node.system_state['location']}\")\n                success = False\n\n        # Check expected held object\n        if 'expected_held_object' in scenario:\n            expected_obj = scenario['expected_held_object']\n            actual_obj = self.node.system_state['held_object']\n            if expected_obj not in str(actual_obj).lower():\n                self.get_logger().warn(f\"Expected held object {expected_obj}, got {actual_obj}\")\n                success = False\n\n        return success\n\n    def get_recent_actions(self):\n        \"\"\"\n        Get recently executed actions (in practice, this would track actual actions)\n        \"\"\"\n        # This would be implemented based on your action tracking system\n        return ['navigate_to', 'detect_object']  # Placeholder\n\n    async def wait_for_task_completion(self, timeout=60):\n        \"\"\"\n        Wait for task completion with timeout\n        \"\"\"\n        start_time = time.time()\n        while (time.time() - start_time) < timeout:\n            if self.node.system_state['task_status'] in ['completed', 'failed', 'idle']:\n                return\n            await asyncio.sleep(0.1)\n\n    def generate_test_report(self):\n        \"\"\"\n        Generate comprehensive test report\n        \"\"\"\n        passed = sum(1 for result in self.test_results if result['success'])\n        total = len(self.test_results)\n\n        report = f\"\"\"\n        === SYSTEM TEST REPORT ===\n        Total Tests: {total}\n        Passed: {passed}\n        Failed: {total - passed}\n        Success Rate: {passed/total*100:.1f}%\n\n        Detailed Results:\n        \"\"\"\n\n        for result in self.test_results:\n            status = \"\u2713\" if result['success'] else \"\u2717\"\n            report += f\"  {status} {result['scenario']}: {result['duration']:.2f}s\"\n            if result['error']:\n                report += f\" - ERROR: {result['error']}\"\n            report += \"\\n\"\n\n        self.get_logger().info(report)\n        return report\n\n    def reset_system_state(self):\n        \"\"\"\n        Reset system to initial state for testing\n        \"\"\"\n        self.node.system_state = {\n            'current_task': None,\n            'task_status': 'idle',\n            'battery_level': 100,\n            'location': 'home_base',\n            'held_object': None,\n            'conversation_active': False\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"practical-example-complete-autonomous-task",children:"Practical Example: Complete Autonomous Task"}),"\n",(0,o.jsx)(n.p,{children:"Let's implement a complete example of the autonomous humanoid performing a complex task:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class CompleteTaskDemo:\n    def __init__(self, humanoid_node):\n        self.humanoid = humanoid_node\n\n    async def run_fetch_and_deliver_demo(self):\n        """\n        Complete demo: Fetch an object and deliver it\n        """\n        self.humanoid.get_logger().info("Starting fetch and deliver demo...")\n\n        # Step 1: Receive voice command\n        command = "Please go to the kitchen, find the red cup, pick it up, and bring it to me"\n\n        self.humanoid.get_logger().info(f"Processing command: {command}")\n\n        # Step 2: Process through VLA pipeline\n        await self.humanoid.process_voice_command_async(command)\n\n        # Step 3: Monitor execution\n        await self.monitor_task_execution()\n\n        # Step 4: Verify completion\n        success = self.verify_task_completion()\n\n        if success:\n            self.humanoid.get_logger().info("Fetch and deliver task completed successfully!")\n        else:\n            self.humanoid.get_logger().warn("Fetch and deliver task encountered issues.")\n\n    async def monitor_task_execution(self):\n        """\n        Monitor task execution in real-time\n        """\n        start_time = time.time()\n        timeout = 120  # 2 minutes timeout\n\n        while (time.time() - start_time) < timeout:\n            current_status = self.humanoid.system_state[\'task_status\']\n\n            if current_status in [\'completed\', \'failed\']:\n                break\n\n            self.humanoid.get_logger().info(f"Task status: {current_status}")\n            await asyncio.sleep(1)  # Check every second\n\n    def verify_task_completion(self):\n        """\n        Verify that the fetch and deliver task was completed\n        """\n        # Check that robot returned to user location\n        # Check that robot is holding the cup\n        # Check that task status is completed\n\n        return (self.humanoid.system_state[\'task_status\'] == \'completed\' and\n                \'cup\' in str(self.humanoid.system_state[\'held_object\']).lower())\n\n    async def run_conversation_demo(self):\n        """\n        Demo of conversational interaction\n        """\n        self.humanoid.get_logger().info("Starting conversation demo...")\n\n        # Simulate multi-turn conversation\n        conversation = [\n            "Hello robot",\n            "Can you go to the kitchen?",\n            "What do you see there?",\n            "Can you pick up the bottle?",\n            "Now bring it to me"\n        ]\n\n        for i, utterance in enumerate(conversation):\n            self.humanoid.get_logger().info(f"Turn {i+1}: {utterance}")\n\n            # Process each utterance\n            command_msg = String()\n            command_msg.data = utterance\n            self.humanoid.voice_command_sub.publish(command_msg)\n\n            # Wait for response\n            await asyncio.sleep(3)  # Allow time for processing\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.p,{children:"Optimizing the complete system for real-time performance:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class PerformanceOptimizer:\n    def __init__(self, node):\n        self.node = node\n        self.performance_metrics = {}\n        self.optimization_strategies = [\n            self.optimize_vision_processing,\n            self.optimize_llm_queries,\n            self.optimize_action_execution,\n            self.optimize_communication\n        ]\n\n    def optimize_vision_processing(self):\n        """\n        Optimize vision processing for real-time performance\n        """\n        # Use smaller model for real-time processing\n        # Implement frame skipping\n        # Use GPU acceleration\n        # Optimize image resolution\n        pass\n\n    def optimize_llm_queries(self):\n        """\n        Optimize LLM query performance\n        """\n        # Use smaller models for simple tasks\n        # Implement query caching\n        # Use streaming responses\n        # Batch similar queries\n        pass\n\n    def optimize_action_execution(self):\n        """\n        Optimize action execution performance\n        """\n        # Pre-compute common trajectories\n        # Use action parallelization where safe\n        # Implement action cancellation\n        # Optimize control loop frequency\n        pass\n\n    def optimize_communication(self):\n        """\n        Optimize ROS 2 communication\n        """\n        # Use appropriate QoS settings\n        # Implement message compression\n        # Use services for synchronous calls\n        # Use actions for long-running tasks\n        pass\n\n    def monitor_performance(self):\n        """\n        Monitor system performance metrics\n        """\n        import psutil\n        import time\n\n        metrics = {\n            \'timestamp\': time.time(),\n            \'cpu_percent\': psutil.cpu_percent(),\n            \'memory_percent\': psutil.virtual_memory().percent,\n            \'disk_io\': psutil.disk_io_counters(),\n            \'network_io\': psutil.net_io_counters()\n        }\n\n        # Add custom metrics\n        metrics[\'robot_status_frequency\'] = self.get_message_frequency(\'/robot_status\')\n        metrics[\'vision_processing_time\'] = self.get_average_vision_time()\n        metrics[\'llm_response_time\'] = self.get_average_llm_time()\n\n        self.performance_metrics[metrics[\'timestamp\']] = metrics\n        return metrics\n\n    def get_message_frequency(self, topic_name):\n        """\n        Get message frequency for a topic (simplified)\n        """\n        # In practice, this would track message rates\n        return 10.0  # Placeholder\n\n    def get_average_vision_time(self):\n        """\n        Get average vision processing time\n        """\n        # In practice, this would track processing times\n        return 0.1  # Placeholder (100ms)\n\n    def get_average_llm_time(self):\n        """\n        Get average LLM response time\n        """\n        # In practice, this would track response times\n        return 2.0  # Placeholder (2 seconds)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices-for-vla-integration",children:"Best Practices for VLA Integration"}),"\n",(0,o.jsx)(n.p,{children:"Key best practices for integrating the complete VLA system:"}),"\n",(0,o.jsx)(n.h3,{id:"1-system-architecture",children:"1. System Architecture"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use modular design with clear interfaces between components"}),"\n",(0,o.jsx)(n.li,{children:"Implement proper error handling and recovery mechanisms"}),"\n",(0,o.jsx)(n.li,{children:"Design for scalability and maintainability"}),"\n",(0,o.jsx)(n.li,{children:"Ensure component independence for easier debugging"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-real-time-performance",children:"2. Real-Time Performance"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Optimize each component for real-time execution"}),"\n",(0,o.jsx)(n.li,{children:"Implement appropriate timeouts for all operations"}),"\n",(0,o.jsx)(n.li,{children:"Use asynchronous processing where possible"}),"\n",(0,o.jsx)(n.li,{children:"Monitor system performance continuously"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"3-safety-and-reliability",children:"3. Safety and Reliability"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement comprehensive safety checks"}),"\n",(0,o.jsx)(n.li,{children:"Use fail-safe mechanisms for critical operations"}),"\n",(0,o.jsx)(n.li,{children:"Validate all inputs and outputs"}),"\n",(0,o.jsx)(n.li,{children:"Include emergency stop capabilities"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"4-user-experience",children:"4. User Experience"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Provide clear feedback during task execution"}),"\n",(0,o.jsx)(n.li,{children:"Handle ambiguous commands gracefully"}),"\n",(0,o.jsx)(n.li,{children:"Maintain conversation context appropriately"}),"\n",(0,o.jsx)(n.li,{children:"Use natural, helpful responses"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"common-pitfalls-and-solutions",children:"Common Pitfalls and Solutions"}),"\n",(0,o.jsx)(n.h3,{id:"1-integration-challenges",children:"1. Integration Challenges"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Components don't communicate properly"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Use standardized message formats and thorough testing"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-performance-issues",children:"2. Performance Issues"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": System too slow for real-time interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Optimize critical paths and use appropriate hardware"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"3-error-propagation",children:"3. Error Propagation"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Errors in one component affect the whole system"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Implement proper error isolation and recovery"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"4-resource-management",children:"4. Resource Management"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Memory or computation resource exhaustion"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Implement proper resource monitoring and management"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this capstone chapter, you learned:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\u2705 How to integrate all VLA components into a complete system"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Building an autonomous humanoid with voice interaction"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 LLM-based planning and execution coordination"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Vision-language integration for object understanding"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Complete action execution and control systems"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Isaac Sim integration for simulation and testing"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Comprehensive system testing and validation"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Performance optimization for real-time operation"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"Congratulations! You've completed the Vision-Language-Action Robotics module. You now have the knowledge to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Build complete VLA systems that understand voice commands"}),"\n",(0,o.jsx)(n.li,{children:"Plan complex tasks using LLMs"}),"\n",(0,o.jsx)(n.li,{children:"Integrate vision-language understanding for object interaction"}),"\n",(0,o.jsx)(n.li,{children:"Execute precise robot actions and manipulations"}),"\n",(0,o.jsx)(n.li,{children:"Create conversational interfaces for natural interaction"}),"\n",(0,o.jsx)(n.li,{children:"Test and validate complete robotic systems"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This capstone project demonstrates the full potential of VLA systems, showing how vision, language, and action can work together to create truly intelligent and helpful robots."}),"\n",(0,o.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://github.com/example/vla-humanoid",children:"Complete VLA System Repository"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/",children:"Isaac Sim Documentation"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://navigation.ros.org/",children:"ROS 2 Robot Development Guide"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2308.16895",children:"Large Language Models for Robotics"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://humanoid.ros.org/",children:"Humanoid Robotics Development"})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"project-ideas-for-further-development",children:"Project Ideas for Further Development"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-Robot Coordination"}),": Extend the system to coordinate multiple robots"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Learning from Demonstration"}),": Add capability to learn new tasks from human demonstration"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Advanced Manipulation"}),": Implement more sophisticated grasping and manipulation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Long-term Autonomy"}),": Add capabilities for long-term operation and learning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Social Interaction"}),": Enhance conversational abilities for more natural interaction"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const o={},i=s.createContext(o);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);