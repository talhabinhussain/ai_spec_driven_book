"use strict";(self.webpackChunkai_native_book=self.webpackChunkai_native_book||[]).push([[353],{6795:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4-vla/chapter-4-2-whisper-speech","title":"Chapter 4.2: Speech Recognition with Whisper","description":"Overview","source":"@site/docs/module-4-vla/chapter-4-2-whisper-speech.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-4-2-whisper-speech","permalink":"/ai_spec_driven_book/docs/module-4-vla/chapter-4-2-whisper-speech","draft":false,"unlisted":false,"editUrl":"https://github.com/talhabinhussain/ai_spec_driven_book/tree/main/docs/module-4-vla/chapter-4-2-whisper-speech.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4.1: Introduction to Vision-Language-Action Models","permalink":"/ai_spec_driven_book/docs/module-4-vla/chapter-4-1-vla-intro"},"next":{"title":"Chapter 4.3: Language Models for Robot Planning","permalink":"/ai_spec_driven_book/docs/module-4-vla/chapter-4-3-llm-planning"}}');var o=i(4848),t=i(8453);const r={sidebar_position:3},s="Chapter 4.2: Speech Recognition with Whisper",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Understanding Whisper Architecture",id:"understanding-whisper-architecture",level:2},{value:"Key Features of Whisper",id:"key-features-of-whisper",level:3},{value:"Whisper Integration with ROS 2",id:"whisper-integration-with-ros-2",level:2},{value:"Real-Time Speech Processing",id:"real-time-speech-processing",level:2},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:2},{value:"Practical Example: Voice-Controlled Robot",id:"practical-example-voice-controlled-robot",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Model Selection",id:"1-model-selection",level:3},{value:"2. Audio Quality",id:"2-audio-quality",level:3},{value:"3. Context Awareness",id:"3-context-awareness",level:3},{value:"4. Error Handling",id:"4-error-handling",level:3},{value:"Common Pitfalls",id:"common-pitfalls",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-42-speech-recognition-with-whisper",children:"Chapter 4.2: Speech Recognition with Whisper"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI's Whisper model has revolutionized automatic speech recognition (ASR) by providing robust, multilingual speech-to-text capabilities. In this chapter, we'll explore how to integrate Whisper with robotic systems to enable voice command interfaces. You'll learn to build real-time speech processing pipelines that can understand and respond to natural language commands in robotic environments."}),"\n",(0,o.jsx)(n.h2,{id:"understanding-whisper-architecture",children:"Understanding Whisper Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Whisper is a transformer-based automatic speech recognition model trained on a large dataset of diverse audio. It excels at handling various accents, background noise, and multiple languages, making it ideal for robotic applications."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Audio Input] --\x3e B[Feature Extraction]\n    B --\x3e C[Whisper Encoder]\n    C --\x3e D[Whisper Decoder]\n    D --\x3e E[Text Output]\n    E --\x3e F[Command Parser]\n"})}),"\n",(0,o.jsx)(n.h3,{id:"key-features-of-whisper",children:"Key Features of Whisper"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multilingual Support"}),": Supports 99+ languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Handles background noise and accents"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Timestamps"}),": Provides word-level timing information"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speaker Identification"}),": Can distinguish between speakers"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Punctuation"}),": Automatically adds punctuation and capitalization"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"whisper-integration-with-ros-2",children:"Whisper Integration with ROS 2"}),"\n",(0,o.jsx)(n.p,{children:"To integrate Whisper with ROS 2, we need to create a speech processing pipeline that can handle real-time audio input and generate text commands:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nimport whisper\nimport numpy as np\nimport torch\nimport pyaudio\nimport wave\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_node\')\n\n        # Load Whisper model\n        self.model = whisper.load_model("base.en")  # or "base", "small", "medium", "large"\n\n        # Audio subscription\n        self.audio_sub = self.create_subscription(\n            AudioData, \'audio_input\', self.audio_callback, 10\n        )\n\n        # Command publisher\n        self.command_pub = self.create_publisher(String, \'voice_command\', 10)\n\n        # Real-time audio processing\n        self.setup_audio_stream()\n\n        self.get_logger().info("Whisper node initialized")\n\n    def setup_audio_stream(self):\n        """\n        Set up real-time audio capture for continuous processing\n        """\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=16000,\n            input=True,\n            frames_per_buffer=8192\n        )\n\n        # Start continuous audio processing\n        self.process_audio_continuously()\n\n    def audio_callback(self, msg):\n        """\n        Handle audio data from ROS topic\n        """\n        # Convert audio data to numpy array\n        audio_array = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Transcribe audio using Whisper\n        result = self.model.transcribe(audio_array)\n        text = result["text"].strip()\n\n        if text:\n            # Publish recognized command\n            command_msg = String()\n            command_msg.data = text\n            self.command_pub.publish(command_msg)\n            self.get_logger().info(f"Recognized: {text}")\n\n    def process_audio_continuously(self):\n        """\n        Process audio in real-time for continuous command recognition\n        """\n        def audio_thread():\n            while rclpy.ok():\n                # Read audio data from stream\n                audio_data = self.stream.read(8192)\n                audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n\n                # Only process if audio level is above threshold (voice activity detection)\n                if np.max(np.abs(audio_array)) > 0.01:  # Simple VAD threshold\n                    result = self.model.transcribe(audio_array)\n                    text = result["text"].strip()\n\n                    if text:\n                        command_msg = String()\n                        command_msg.data = text\n                        self.command_pub.publish(command_msg)\n                        self.get_logger().info(f"Voice command: {text}")\n\n        import threading\n        threading.Thread(target=audio_thread, daemon=True).start()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-speech-processing",children:"Real-Time Speech Processing"}),"\n",(0,o.jsx)(n.p,{children:"For real-time applications, we need to optimize Whisper for low latency while maintaining accuracy:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport queue\nimport threading\n\nclass RealTimeWhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'realtime_whisper_node\')\n\n        self.model = whisper.load_model("small.en")\n        self.audio_queue = queue.Queue()\n\n        # Setup audio stream\n        self.setup_realtime_processing()\n\n        # Command publisher\n        self.command_pub = self.create_publisher(String, \'voice_command\', 10)\n\n    def setup_realtime_processing(self):\n        """\n        Setup real-time audio processing with buffering\n        """\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=16000,\n            input=True,\n            frames_per_buffer=4096\n        )\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio_stream, daemon=True)\n        self.processing_thread.start()\n\n    def process_audio_stream(self):\n        """\n        Continuously process audio stream with buffering\n        """\n        buffer = np.array([])\n\n        while rclpy.ok():\n            # Read audio chunk\n            data = self.stream.read(4096)\n            chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Add to buffer\n            buffer = np.concatenate([buffer, chunk])\n\n            # Process when buffer reaches threshold\n            if len(buffer) >= 16000:  # 1 second of audio\n                # Only process if significant audio energy\n                if np.max(np.abs(buffer)) > 0.01:\n                    # Transcribe the buffer\n                    result = self.model.transcribe(buffer, language=\'en\')\n                    text = result["text"].strip()\n\n                    if text:\n                        self.publish_command(text)\n\n                # Keep last 0.5 seconds for continuity\n                buffer = buffer[-8000:]\n\n    def publish_command(self, text):\n        """\n        Publish recognized command with confidence scoring\n        """\n        command_msg = String()\n        command_msg.data = text\n        self.command_pub.publish(command_msg)\n        self.get_logger().info(f"Command: {text}")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"A complete voice command system includes preprocessing, recognition, and command parsing:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class VoiceCommandProcessor:\n    def __init__(self):\n        self.whisper_model = whisper.load_model(\"base.en\")\n        self.command_patterns = self.define_command_patterns()\n\n    def define_command_patterns(self):\n        \"\"\"\n        Define common command patterns for robotics\n        \"\"\"\n        return {\n            'navigation': [\n                r'move to (.+)',\n                r'go to (.+)',\n                r'go to the (.+)',\n                r'walk to (.+)',\n                r'navigate to (.+)'\n            ],\n            'manipulation': [\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'pick the (.+) up',\n                r'lift the (.+)',\n                r'get the (.+)'\n            ],\n            'object_interaction': [\n                r'open (.+)',\n                r'close (.+)',\n                r'turn on (.+)',\n                r'turn off (.+)',\n                r'switch (.+)'\n            ]\n        }\n\n    def process_voice_command(self, audio_input):\n        \"\"\"\n        Complete pipeline: audio -> text -> command -> action\n        \"\"\"\n        # Step 1: Transcribe audio to text\n        if isinstance(audio_input, np.ndarray):\n            result = self.whisper_model.transcribe(audio_input)\n        else:\n            result = self.whisper_model.transcribe(audio_input)  # file path\n\n        text = result[\"text\"].strip()\n\n        # Step 2: Parse command structure\n        parsed_command = self.parse_command_structure(text)\n\n        # Step 3: Validate and sanitize command\n        if self.validate_command(parsed_command):\n            return parsed_command\n        else:\n            return None\n\n    def parse_command_structure(self, text):\n        \"\"\"\n        Parse natural language command into structured format\n        \"\"\"\n        import re\n\n        # Normalize text\n        text = text.lower().strip()\n\n        # Identify command type and parameters\n        for cmd_type, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text)\n                if match:\n                    return {\n                        'type': cmd_type,\n                        'parameters': match.groups(),\n                        'original': text,\n                        'confidence': result.get('confidence', 0.8)  # Assuming high confidence\n                    }\n\n        # If no pattern matches, return as general command\n        return {\n            'type': 'general',\n            'parameters': [text],\n            'original': text,\n            'confidence': 0.5\n        }\n\n    def validate_command(self, command):\n        \"\"\"\n        Validate command safety and feasibility\n        \"\"\"\n        if command['confidence'] < 0.7:\n            return False  # Low confidence\n\n        # Add safety checks here\n        if 'kill' in command['original'] or 'destroy' in command['original']:\n            return False  # Safety filter\n\n        return True\n"})}),"\n",(0,o.jsx)(n.h2,{id:"practical-example-voice-controlled-robot",children:"Practical Example: Voice-Controlled Robot"}),"\n",(0,o.jsx)(n.p,{children:"Let's build a complete example that combines Whisper with ROS 2 for voice-controlled robot navigation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class VoiceControlledRobot(Node):\n    def __init__(self):\n        super().__init__('voice_controlled_robot')\n\n        # Whisper model\n        self.whisper = whisper.load_model(\"base.en\")\n\n        # ROS 2 interfaces\n        self.command_sub = self.create_subscription(\n            String, 'voice_command', self.command_callback, 10\n        )\n        self.nav_goal_pub = self.create_publisher(String, 'navigation_goal', 10)\n        self.manipulation_pub = self.create_publisher(String, 'manipulation_command', 10)\n\n        # Command processor\n        self.command_processor = VoiceCommandProcessor()\n\n        self.get_logger().info(\"Voice controlled robot initialized\")\n\n    def command_callback(self, msg):\n        \"\"\"\n        Process incoming voice commands\n        \"\"\"\n        command_text = msg.data\n\n        # Parse the command\n        parsed_command = self.command_processor.parse_command_structure(command_text)\n\n        if parsed_command['confidence'] > 0.7:\n            self.execute_command(parsed_command)\n        else:\n            self.get_logger().warn(f\"Low confidence command: {command_text}\")\n\n    def execute_command(self, command):\n        \"\"\"\n        Execute parsed command based on type\n        \"\"\"\n        cmd_type = command['type']\n        params = command['parameters']\n\n        if cmd_type == 'navigation':\n            goal = params[0]  # Location to navigate to\n            nav_msg = String()\n            nav_msg.data = f\"navigate_to:{goal}\"\n            self.nav_goal_pub.publish(nav_msg)\n            self.get_logger().info(f\"Navigating to: {goal}\")\n\n        elif cmd_type == 'manipulation':\n            object_name = params[0]  # Object to manipulate\n            manip_msg = String()\n            manip_msg.data = f\"pick_up:{object_name}\"\n            self.manipulation_pub.publish(manip_msg)\n            self.get_logger().info(f\"Attempting to pick up: {object_name}\")\n\n        elif cmd_type == 'object_interaction':\n            target = params[0]  # Object to interact with\n            # Determine action based on original command\n            if 'open' in command['original']:\n                action = 'open'\n            elif 'close' in command['original']:\n                action = 'close'\n            elif 'turn on' in command['original']:\n                action = 'turn_on'\n            elif 'turn off' in command['original']:\n                action = 'turn_off'\n            else:\n                action = 'interact'\n\n            interaction_msg = String()\n            interaction_msg.data = f\"{action}:{target}\"\n            self.manipulation_pub.publish(interaction_msg)\n            self.get_logger().info(f\"Interacting with {target} - {action}\")\n\n        else:\n            self.get_logger().info(f\"General command: {command['original']}\")\n"})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.p,{children:"For real-time applications, consider these optimization strategies:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class OptimizedWhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'optimized_whisper_node\')\n\n        # Use smaller model for faster inference\n        self.model = whisper.load_model("tiny.en")\n\n        # GPU acceleration if available\n        if torch.cuda.is_available():\n            self.model = self.model.cuda()\n\n        # Audio processing optimizations\n        self.setup_optimized_audio()\n\n        # Command caching to avoid duplicate processing\n        self.command_cache = {}\n\n    def setup_optimized_audio(self):\n        """\n        Setup optimized audio processing pipeline\n        """\n        # Use lower sample rate for faster processing\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=8000,  # Lower rate for faster processing\n            input=True,\n            frames_per_buffer=2048  # Smaller buffer for lower latency\n        )\n\n    def transcribe_with_cache(self, audio_data, cache_duration=5.0):\n        """\n        Transcribe audio with caching to avoid duplicate processing\n        """\n        import hashlib\n        import time\n\n        # Create hash of audio data for caching\n        audio_hash = hashlib.md5(audio_data.tobytes()).hexdigest()\n        current_time = time.time()\n\n        # Check if we have cached result\n        if audio_hash in self.command_cache:\n            cached_result, timestamp = self.command_cache[audio_hash]\n            if current_time - timestamp < cache_duration:\n                return cached_result\n\n        # Process new audio\n        result = self.model.transcribe(audio_data)\n\n        # Cache the result\n        self.command_cache[audio_hash] = (result, current_time)\n\n        return result\n'})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsx)(n.h3,{id:"1-model-selection",children:"1. Model Selection"}),"\n",(0,o.jsx)(n.p,{children:"Choose the appropriate Whisper model size based on your hardware constraints:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tiny"}),": Fastest, lowest accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"base"}),": Good balance of speed and accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"small"}),": Better accuracy, moderate speed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"medium"}),": High accuracy, slower"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"large"}),": Highest accuracy, slowest"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-audio-quality",children:"2. Audio Quality"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use high-quality microphones for better recognition"}),"\n",(0,o.jsx)(n.li,{children:"Implement noise reduction filters"}),"\n",(0,o.jsx)(n.li,{children:"Consider beamforming for directional audio capture"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"3-context-awareness",children:"3. Context Awareness"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Provide context to the model when possible"}),"\n",(0,o.jsx)(n.li,{children:"Use language models to disambiguate commands"}),"\n",(0,o.jsx)(n.li,{children:"Implement dialogue management for multi-turn conversations"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"4-error-handling",children:"4. Error Handling"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Always validate command confidence scores"}),"\n",(0,o.jsx)(n.li,{children:"Implement fallback mechanisms for unrecognized commands"}),"\n",(0,o.jsx)(n.li,{children:"Provide feedback to users about command recognition status"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"common-pitfalls",children:"Common Pitfalls"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High Latency"}),": Large models can introduce significant processing delays"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Background Noise"}),": Poor audio quality can degrade recognition accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Command Ambiguity"}),": Natural language commands may be ambiguous"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Usage"}),": Whisper models can be computationally expensive"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\u2705 How to integrate OpenAI Whisper with ROS 2 for speech recognition"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Real-time audio processing techniques for voice commands"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Command parsing and validation strategies"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Performance optimization for robotic applications"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Building complete voice-controlled robot systems"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Best practices for robust speech recognition"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"Now that you can process voice commands, let's explore how to use Large Language Models for planning robot actions based on natural language input."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Continue to:"})," ",(0,o.jsx)(n.a,{href:"./chapter-4-3-llm-planning",children:"Chapter 4.3: Language Models for Robot Planning \u2192"})]}),"\n",(0,o.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper Documentation"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://cdn.openai.com/papers/whisper.pdf",children:"Whisper Paper"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2203.09893",children:"Real-Time Speech Recognition Techniques"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/9812345",children:"Voice Command Systems for Robotics"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://pyaudio.readthedocs.io/",children:"PyAudio Documentation"})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var a=i(6540);const o={},t=a.createContext(o);function r(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);